import json
import pandas as pd
from urllib.parse import urlparse

# â”€â”€â”€ ğ—£ğ—®ğ˜ğ—µ ğ˜ğ—¼ ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—™ğ—œğ—Ÿğ—˜ â”€â”€â”€
# Use a Linux-style path, which works natively in WSL.
har_path = "projects/auto-stack/docs/reference/dev-localhost.har"

# â”€â”€â”€ ğ—Ÿğ—¼ğ—®ğ—± ğ—§ğ—µğ—² ğ—›ğ—”ğ—¥ â”€â”€â”€
with open(har_path, "r", encoding="utf-8") as f:
    har_data = json.load(f)

entries = har_data.get("log", {}).get("entries", [])
print(f"âœ… Loaded {len(entries)} entries from HAR")

# â”€â”€â”€ ğ—£ğ—®ğ—¿ğ˜€ğ—² ğ—²ğ—»ğ˜ğ—¿ğ—¶ğ—²ğ˜€ â”€â”€â”€
parsed = []
for entry in entries:
    req = entry.get("request", {})
    resp = entry.get("response", {})
    timings = entry.get("timings", {})

    url = req.get("url", "")
    domain = urlparse(url).netloc or "n/a"
    mime_full = resp.get("content", {}).get("mimeType", "unknown")
    mime = mime_full.split("/")[0] if "/" in mime_full else mime_full
    size = resp.get("content", {}).get("size", 0)
    total_time = entry.get("time", 0)
    ttfb = timings.get("send", 0) + timings.get("wait", 0)
    status = resp.get("status", 0)
    method = req.get("method", "")

    parsed.append({
        "URL": url,
        "Domain": domain,
        "MimeType": mime,
        "SizeBytes": size,
        "TotalTimeMs": total_time,
        "TTFBms": ttfb,
        "Status": status,
        "Method": method
    })

df = pd.DataFrame(parsed)
if df.empty:
    print("âš ï¸ Parsed DataFrame is emptyâ€”no entries found.")
    exit(1)

# â”€â”€â”€ ğ—–ğ—¼ğ—¿ğ—² ğ—¦ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ˜† â”€â”€â”€
total_requests = len(df)
total_bytes = df["SizeBytes"].sum()
unique_domains = df["Domain"].nunique()
top_domains = df["Domain"].value_counts().head(10)
by_type = df["MimeType"].value_counts()

print("\nğŸ”¹ ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ—¹ ğ—¦ğ˜ğ—®ğ˜ğ˜€:")
print(f"   â€¢ Total requests: {total_requests}")
print(f"   â€¢ Total bytes transferred: {total_bytes} bytes")
print(f"   â€¢ Unique domains contacted: {unique_domains}")

print("\nğŸ”¹ ğ—§ğ—¼ğ—½ 10 ğ——ğ—¼ğ—ºğ—®ğ—¶ğ—»ğ˜€ ğ—¯ğ˜† ğ—–ğ—¼ğ˜‚ğ—»ğ˜:")
print(top_domains.to_string())

print("\nğŸ”¹ ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ ğ—–ğ—¼ğ˜‚ğ—»ğ˜ğ˜€ ğ—¯ğ˜† ğ— ğ—¶ğ—ºğ—² ğ—§ğ˜†ğ—½ğ—²:")
print(by_type.to_string())

# â”€â”€â”€ ğ—Ÿğ—®ğ—¿ğ—´ğ—²ğ˜€ğ˜ ğ—£ğ—®ğ˜†ğ—¹ğ—¼ğ—®ğ—±ğ˜€ â”€â”€â”€
largest = df.nlargest(10, "SizeBytes")[["URL","Domain","SizeBytes","Status","MimeType"]]
print("\nğŸ”¹ ğ—§ğ—¼ğ—½ 10 ğ—Ÿğ—®ğ—¿ğ—´ğ—²ğ˜€ğ˜ ğ—£ğ—®ğ˜†ğ—¹ğ—¼ğ—®ğ—±ğ˜€:")
print(largest.to_string(index=False))

# â”€â”€â”€ ğ—¦ğ—¹ï¿½ï¿½ğ˜„ğ—²ğ˜€ğ˜ ğ—˜ğ—»ğ—±ğ—½ğ—¼ğ—¶ğ—»ğ˜ğ˜€ â”€â”€â”€
slowest = df.nlargest(10, "TotalTimeMs")[["URL","Domain","TotalTimeMs","TTFBms","Status","MimeType"]]
print("\nğŸ”¹ ğ—§ğ—¼ğ—½ 10 ğ—¦ğ—¹ğ—¼ğ˜„ğ—²ğ˜€ğ˜ ğ—˜ğ—»ğ—±ğ—½ğ—¼ğ—¶ğ—»ğ˜ğ˜€:")
print(slowest.to_string(index=False))

# â”€â”€â”€ ğ—˜ğ—¿ğ—¿ğ—¼ğ—¿ ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ˜€ â”€â”€â”€
errors = df[df["Status"] >= 400]
if not errors.empty:
    print("\nğŸ”¹ ğ—˜ğ—¿ğ—¿ğ—¼ğ—¿ ğ—¥ğ—²ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ˜€ (status â‰¥ 400):")
    print(errors[["URL","Domain","Status","MimeType"]].to_string(index=False))
else:
    print("\nğŸ”¹ No 4xx/5xx errors detected.")

# â”€â”€â”€ ğ—£ğ—®ğ—´ğ—² ğ—Ÿğ—¼ğ—®ğ—± ğ—§ğ—¶ğ—ºğ—¶ğ—»ğ—´ğ˜€ â”€â”€â”€
pages = har_data.get("log", {}).get("pages", [])
if pages:
    pt = pages[0].get("pageTimings", {})
    print(f"\nğŸ”¹ Page onContentLoad: {pt.get('onContentLoad')} ms")
    print(f"ğŸ”¹ Page onLoad:        {pt.get('onLoad')} ms")
